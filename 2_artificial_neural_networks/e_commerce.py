# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13M6KkkEO7ekZAX_pgXBj6e2Eo2yyudAn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.utils import shuffle
pd.set_option('display.max_columns', 500)

from google.colab import drive 
drive.mount('/content/gdrive')

#getting rid of categorical variables
df = pd.read_csv('gdrive/My Drive/csvs/ecommerce_data.csv')
df = pd.get_dummies(df, columns=['time_of_day'], drop_first=True)
df = pd.get_dummies(df, columns=['user_action'])

#normalizing numerical variables
scaler = StandardScaler()
df['n_products_viewed'] = scaler.fit_transform(df[['n_products_viewed']].values.astype(float))
df['visit_duration'] = scaler.fit_transform(df[['visit_duration']].values.astype(float))

X = df.values[:, :7]
Y = df.values[:, -4:]

X, Y = shuffle(X, Y)

def sigmoid(X):
  return 1/(1 + np.exp(-X))

def softmax(y):
  ye = np.exp(y)
  ys = np.sum(ye, axis=1, keepdims=True)
  return ye/ys

def feedforward(X, W1, b1, W2, b2):
  Z = sigmoid(X.dot(W1) + b1)
  Y = softmax(Z.dot(W2) + b2)

  return Y, Z

def classification_rate(Y, P):
  n_correct = 0
  n_total = 0

  for i in range(len(Y)):
    n_total += 1

    if Y[i] == P[i]:
      n_correct += 1
    
  return n_correct/n_total

def predict(P_Y_given_X):
    return np.argmax(P_Y_given_X, axis=1)

def cost(T, output):
  return np.sum(T * np.log(output))

def der_w2(Z, T, Y):
  return Z.T.dot(T-Y)

def der_b2(T, Y):
  return (T-Y).sum(axis=0)

def der_w1(X, Z, T, Y, W):
  return X.T.dot((T-Y).dot(W2.T)*(1 - Z*Z))

def der_b1(T, Y, W2, Z):
  return ((T-Y).dot(W2.T)*(1 - Z*Z)).sum(axis=0)

def backpropagation(lr, epochs, X, Y, W1, b1, W2, b2):
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
  train_costs = []
  test_costs = []
  nW1, nW2, nb1, nb2 = W1, W2, b1, b2
  for epoch in range(epochs):
    output_train, hidden_train = feedforward(X_train, nW1, nb1, nW2, nb2)
    output_test, hidden_test = feedforward(X_test, nW1, nb1, nW2, nb2)

    c_train = cost(Y_train, output_train)
    c_test = cost(Y_test, output_test)
    train_costs.append(c_train)
    test_costs.append(c_test)

    if epoch%1000 == 0:
      print(epoch, ': ', c_train, '  ', c_test)

    #gradient ascent!
    nW2 += lr * der_w2(hidden_train, Y_train, output_train)
    nb2 += lr * der_b2(Y_train, output_train)
    nW1 += lr * der_w1(X_train, hidden_train, Y_train, output_train, nW2)
    nb1 += lr * der_b1(Y_train, output_train, nW2, hidden_train)

  print("Final test classification rate: ", classification_rate(predict(Y_test), predict(output_test)))
  print("Final train classification rate: ", classification_rate(predict(Y_train), predict(output_train)))

  legend1, = plt.plot(train_costs, label='train cost')
  legend2, = plt.plot(test_costs, label='test cost')
  plt.legend([legend1, legend2])
  plt.show()

from sklearn.model_selection import train_test_split

M = 5
D = X.shape[1]
K = 4

W1 = np.random.randn(D, M)
b1 = np.zeros(M)
W2 = np.random.randn(M, K)
b2 = np.zeros(K)

backpropagation(10e-3, 10000, X, Y, W1, b1, W2, b2)