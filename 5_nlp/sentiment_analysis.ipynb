{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMmJjxakLOWoqwmsrUHgtUr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-NiQxTYPodo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1a0814a6-9de5-479a-a280-66cbaba78322"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
        "\n",
        "positive_reviews = BeautifulSoup(open('positive.review').read())\n",
        "positive_reviews = positive_reviews.findAll('review_text')\n",
        "negative_reviews = BeautifulSoup(open('negative.review').read())\n",
        "negative_reviews = negative_reviews.findAll('review_text')\n",
        "\n",
        "np.random.shuffle(positive_reviews)\n",
        "np.random.shuffle(negative_reviews)\n",
        "\n",
        "def my_tokenizer(s):\n",
        "    s = s.lower()\n",
        "    tokens = nltk.tokenize.word_tokenize(s)\n",
        "    tokens = [t for t in tokens if len(t) > 2]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfI21fGNRxC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "\n",
        "word_index_map = {}\n",
        "current_index = 0\n",
        "\n",
        "for review in positive_reviews:\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    positive_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "for review in negative_reviews:\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    negative_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOJTKAA5USoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_vector(tokens, label):\n",
        "    x = np.zeros(len(word_index_map)+1)\n",
        "    for t in tokens:\n",
        "        i = word_index_map[t]\n",
        "        x[i] += 1\n",
        "    x /= x.sum()\n",
        "    x[-1] = label\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcZxQef9Uptk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = len(positive_tokenized) + len(negative_tokenized)\n",
        "data = np.zeros((N, len(word_index_map) + 1))\n",
        "i = 0\n",
        "\n",
        "for tokens in positive_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 1)\n",
        "    data[i, :] = xy\n",
        "    i += 1\n",
        "\n",
        "for tokens in negative_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 0)\n",
        "    data[i, :] = xy\n",
        "    i += 1\n",
        "\n",
        "np.random.shuffle(data)\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPMn8Q4gVkUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64955af6-895e-4095-813b-827d8c0a803f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoshjKFHWnaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "e0dba34a-6c03-42f2-8bac-f96554926f99"
      },
      "source": [
        "threshold = 0.7\n",
        "for word, index in word_index_map.items():\n",
        "    weight = model.coef_[0][index]\n",
        "    if weight > threshold or weight < -threshold:\n",
        "        print(word, weight)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sound 0.90642942908951\n",
            "price 2.452030044125941\n",
            "n't -1.944548445655888\n",
            "you 1.0324737007749956\n",
            "month -0.7208810814779492\n",
            "easy 1.5072267155063\n",
            "highly 0.8854056001373386\n",
            "quality 1.3069117164531037\n",
            "love 0.9970779509403283\n",
            "wa -1.453654853996005\n",
            "lot 0.7687760063914272\n",
            "excellent 1.2199409203604985\n",
            "cable 0.7616221958507047\n",
            "doe -1.052638833829289\n",
            "speaker 0.9674517342095352\n",
            "little 0.9330135607088245\n",
            "time -0.7502836529004621\n",
            "perfect 0.8844408223918653\n",
            "support -0.9280758811498445\n",
            "return -1.019719412797275\n",
            "waste -0.829310467762352\n",
            "poor -0.7290065852059215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWA8EnM4XrGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70ca752c-7c5f-4d95-e8fa-fd0b20d3e9ef"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.775\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}